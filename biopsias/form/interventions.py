import pandas as pd
from typing import Callable, Union, Optional, Iterable
import logging
from fuzzywuzzy.fuzz import ratio
import warnings
from pathlib import Path
import json
import numpy as np

from biopsias.config import DIC_CLASSES
from biopsias.form.string_matching import (
    create_dict_fuzzymatch,
    word_n_grams,
    clean_name_hard,
    clean_x_no_deep_learning,
)


def read_google_spreadsheet(
    id_spreadsheet: str = "1mAoGChwEOAvush7vz5pxhvBn3oRJ4QFO", gid: int = 662190549
) -> pd.DataFrame:
    """Read CSV from Google Sheets.

    Parameters
    ----------
    id_spreadsheet : str, optional
        Id of the spreadsheet, by default "1mAoGChwEOAvush7vz5pxhvBn3oRJ4QFO"
    gid : int, optional
        Guid of the spreadsheet, by default 662190549

    Returns
    -------
    pd.DataFrame
        DataFrame containing the data of the spreadsheet.
    """
    df = pd.read_csv(
        f"https://docs.google.com/spreadsheets/d/{id_spreadsheet}/export?gid={gid}&format=csv",
    )

    return df


def load_manual_labeled_interventions() -> pd.DataFrame:
    """Preprocess the data generated by reading the Google spreadsheet in this link:
    https://docs.google.com/spreadsheets/d/1mAoGChwEOAvush7vz5pxhvBn3oRJ4QFO/edit#gid=662190549

    Returns
    -------
    pd.DataFrame
        Data processed.
    """
    csv_path = "./data/clean/manual_interventions_annotated.csv"
    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        print("Loading Manual Annotations from Google Sheets...")
        df = read_google_spreadsheet(
            id_spreadsheet="1mAoGChwEOAvush7vz5pxhvBn3oRJ4QFO", gid=662190549
        )
        df.to_csv(csv_path, index=False)
        print(f"File saved into: {csv_path}")

    df = df.drop(["value_count", "Tipo Intervención Kmorebi"], axis=1)

    df = df.rename(
        columns={"Tipo Intervención Texto": "x", "Tipo de intervención corregido": "y"}
    )

    df["y"] = df["y"].apply(clean_name_hard)

    renames = {
        "mastectomia radical modificada": "mrm",
        "mastectomia radical": "mr",
        "mastectomia simple": "ms",
        "no mama": "invalid",
        "ambiguo": "invalid",
        "eliminar": "invalid",
    }

    for name, replace in renames.items():
        df["y"] = df["y"].str.replace(name, replace)

    df["y"] = df["y"].apply(lambda x: sorted((i.strip() for i in x.split("+"))))

    assert len(df) == len(
        df["x"].drop_duplicates()
    ), "There are duplicated interventions texts."

    return df


def get_labels(
    intervention: str,
    scorer: Callable = ratio,
    score_cutoff: int = 90,
    dic_classes: Optional[dict[str, set[str]]] = None,
) -> list[str]:
    """Classify an intervention in multi label using fuzzy match.
    First of all, the order of the classes is checked and the data is preprocessed to work with fuzzy wuzzy.
    After that, the match is tryed using every choice, subdividing the intervention in n-grams.
    When a n-gram makes a match it is deleted from the intervention string and
    this new string is passed again througth the matching process.
    The algorithm finish when after trying to match every class there aren't new results.

    Parameters
    ----------
    intervention : str
        Text to classify
    scorer : Callable, optional
        Function that claculate the metric to measure similarity, by default ratio.
    score_cutoff : int, optional
        Threshold to determine whether or not select a class, by default 90
    dic_classes : Optional[dict[str, set[str]]], optional
        Dictionary that contains the classes names and their choices, by default None

    Returns
    -------
    list[str]
        List of labels which the text belongs.
    """

    if dic_classes is None:
        dic_classes = DIC_CLASSES

    # Check if the classes dict is correct ordered
    order_needed = ["mrm", "mr", "ms"]
    assert [
        val for val in dic_classes.keys() if val in order_needed
    ] == order_needed, (
        f'Order is not correct, "mastectomias" must follow this order: {order_needed}.'
    )

    global_matches = []

    # Needed to work with fuzzy wuzzy
    # THIS PROBABLY WILL BE ADDED TO `clean_name_hard`
    CHARS_DELETE = "/+();"
    for c in CHARS_DELETE:
        intervention = intervention.replace(c, "").strip()

    # we only look for labels if the string is non-empty
    intervention = intervention.strip()
    new_match = len(intervention) > 0

    # runs until no new match is found
    while new_match:
        matches = []
        new_match = False

        # try to find any of the labels, if it is found, it deleted the associated substring from the string
        for term, choices in dic_classes.items():

            # N will depend of the sizes of the choices
            choices_size = [len(c.split()) for c in choices]
            n_range = range(max(min(choices_size) - 1, 1), max(choices_size) + 2)

            # EVALUATE MATCH CHOICE BY CHOICE
            for choice in choices:
                intervention_ngrams = word_n_grams(intervention, n_range)

                # Select only not empty ngrams
                intervention_ngrams = [
                    ngram for ngram in intervention_ngrams if len(ngram) > 0
                ]

                dict_matches = create_dict_fuzzymatch(
                    intervention_ngrams,
                    [choice],
                    scorer=scorer,
                    score_cutoff=score_cutoff,
                )

                if len(dict_matches) > 0:
                    # print(intervention,dict_matches)
                    MATCH_STRING = list(dict_matches.keys())[0].strip()

                    # Así se evita meter la string vacía
                    if len(term) > 0:
                        matches.append(term)

                    # White spaces are deleted
                    intervention = " ".join(
                        intervention.replace(MATCH_STRING, "").strip().split()
                    )
                    new_match = len(intervention) > 0

                    break

        global_matches.extend(matches)

    global_matches.sort()

    return global_matches


def custom_accuracy(y_true: pd.Series, y_pred: pd.Series) -> tuple[pd.Series, float]:
    """Compute the accuracy of a multi-label classifier where the output is a list of labels.
    The prediction is valid iff all labels match.


    Parameters
    ----------
    y_true : pd.Series
        Ground thruth data.
    y_pred : pd.Series
        Predicted data.

    Returns
    -------
    tuple[pd.Series,float]
        Tuple containing the `mask` to index the correct values and the `score` of the accuracy.
    """
    mask = y_true.map(sorted) == y_pred.map(sorted)
    accuracy = mask.mean()

    return mask, accuracy


def validation(
    df: pd.DataFrame, score_cutoff: int, scorer: Callable, return_errors: bool = False
) -> Union[float, tuple[float, pd.DataFrame]]:
    """Validate on dataset `df` to get the accuracy of a `scorer` with an scpecific `score_cutoff`.


    Parameters
    ----------
    df : pd.DataFrame
        Data frame to collect the data (`x` and `y`).
    score_cutoff : int
        Threshold to determine whether or not consider a match.
    scorer : Callable
        Scorer algorithm to select match.
    return_errors : bool, optional
        If it is `True` will return accuracy errors, if it is `False` wont, by default False

    Returns
    -------
    float
        Accuracy of the validation and accuracy errors, if it is indicated.
    """
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            message=r"Element\s.*\shas\sno\smatch\swith\sthreshold\s\d{0,2}",
            category=UserWarning,
        )
        y_true = df["y"]
        y_pred = df["x"].apply(get_labels, scorer=scorer, score_cutoff=score_cutoff)

        mask, accuracy = custom_accuracy(y_true, y_pred)

        if return_errors:
            df["y_pred"] = y_pred

            return accuracy, df[~mask][["x", "y", "y_pred"]]
        else:
            return accuracy


def load_json_as_df(json_path: Union[Path, str]) -> pd.DataFrame:
    """Load JSON file, with several levels, as pandas Dataframe.
    These levels are flatten to save the data.

    Parameters
    ----------
    form_parsed : Union[Path, str]
        Path to JSON file to be parsed.
        It must have the following schema: `{index -> {column -> value}`,
        where `value` can be another dictionary.

    Returns
    -------
    pd.Dataframe
        Dataframe containing all the fields of the JSON file.
    """
    with open(json_path, "r") as f:
        all_dicts = json.load(f)

    dfs = []
    for ide, data in all_dicts.items():
        df = pd.json_normalize(data)
        df["id"] = ide
        dfs.append(df)

    df_json = pd.concat(dfs).set_index("id", drop=True)
    # sort columns: first w/o nesting. all string to be comparable
    cols_str = map(str, df_json.columns)
    sorted_cols = sorted(sorted(cols_str), key=lambda x: x.count(".") > 0)
    return df_json[sorted_cols]


def load_dataset_interventions(
    biopsias_data_path: Union[Path, str],
    form_parsed_path: Union[Path, str],
    x_cols: Iterable[str] = ("Datos clínicos", "Diagnóstico", "Macro"),
) -> pd.DataFrame:
    """Load X and Y data from CSV file and the parsed forms, using manual anotated data.

    Parameters
    ----------
    biopsias_data_path : Union[Path,str]
        Path to parquet file that contains all the data related with biopsias studies.
    form_parsed_path : Union[Path, str]
        Path to JSON file containing the parsed form.
    x_cols : Iterable[str], optional
        Columns to generate the X data, by default ("Datos clínicos", "Diagnóstico", "Macro")

    Returns
    -------
    pd.DataFrame
        Dataframe that contains the X separated by columns and the Y data.
    """

    # Read all biopsias data
    df = pd.read_parquet(biopsias_data_path)

    # Rename column names and setting the study as index
    renames = {col: f"X_{clean_name_hard(col).replace(' ','_')}" for col in x_cols}
    result_df = df[list(x_cols) + ["Estudio"]].rename(columns=renames)
    result_df = result_df.set_index("Estudio")

    # Load the labels manually annotated and convert it to a pd.Series
    labels = load_manual_labeled_interventions()[["x", "y"]]
    labels = labels.set_index("x", verify_integrity=True)["y"]

    # Load formed JSON as a Dataframe and replace the intervention text by the manual annotated labels
    result_df["intervention_raw"] = load_json_as_df(form_parsed_path)[
        "intervention_raw"
    ]
    result_df["y"] = result_df["intervention_raw"].map(labels)

    return result_df


def interventions_value_counts(parsed_forms: dict[str, str]) -> pd.DataFrame:
    """Generate information of possible interventions values.

    Parameters
    ----------
    parsed_forms : dict
        Forms parsed.

    Returns
    -------
    pd.DataFrame
        Table containing value counts and predictions, for every possible intervention value.
    """
    intervenciones = pd.Series(
        form["intervention_raw"]
        for form in parsed_forms.values()
        if "intervention_raw" in form.keys()
    )
    counts_df = intervenciones.value_counts(dropna=True).rename("counts").to_frame()

    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            message=r"Element\s.*\shas\sno\smatch\swith\sthreshold\s\d{0,2}",
            category=UserWarning,
        )
        counts_df["classes"] = counts_df.index.map(get_labels)

    return (
        counts_df.reset_index()
        .rename(columns={"index": "intervention"})
        .sort_values("intervention")
        .sort_values("classes", key=lambda ser: ser.str.len(), ignore_index=True)
    )


# TRAINING FUNCTIONS
def intervention_tokenizer(text: str) -> str:
    """Interventions tokenizer.

    Parameters
    ----------
    text : str
        Input text.

    Returns
    -------
    str
        Text tokenized.
    """
    # Habrá que sustituirlo en un futuro
    return text.split()


def exist_mandatory_intervention(
    labels_list: Iterable[str],
    mandatory_labels: Iterable[str] = (
        "tumorectomia",
        "ms",
        "mrm",
        "mr",
        "cuadrantectomia",
    ),
) -> bool:
    """Determine if the list of labels passed has at least one of the mandatory labels.

    Parameters
    ----------
    labels_list : Iterable[str]
        Labels to test if it is valid
    mandatory_labels : Iterable[str], optional
        Labels that must appear in the list, by default ( "tumorectomia", "ms", "mrm", "mr", "cuadrantectomia", )

    Returns
    -------
    bool
        Whether it is a valid list of labels or not.
    """
    return not set(mandatory_labels).isdisjoint(labels_list)
